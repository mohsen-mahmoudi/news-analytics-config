server:
  port: 8181

retry-config:
  initial-interval-ms: 1000 # 1 second
  max-interval-ms: 10000 # 10 seconds
  multiplier: 2.0 # exponential backoff
  max-attempts: 3 # 3 attempts
  sleep-time-ms: 2000 # 2 seconds sleep time

kafka-config:
  bootstrap-servers: localhost:19092, localhost:29092, localhost:39092
  schema-registry-url-key: schema.registry.url
  schema-registry-url: http://localhost:8081
  topic-name: news-topic
  topic-names-to-create:
    - news-topic
  num-of-partitions: 3 # 3 partitions
  replication-factor: 3 # 3 brokers

kafka-consumer-config:
  key-deserializer: org.apache.kafka.common.serialization.LongDeserializer
  value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
  consumer-group-id: news-topic-consumer
  auto-offset-reset: earliest # start from the beginning of the topic
  specific-avro-reader-key: specific.avro.reader
  specific-avro-reader: true
  batch-listener: true # means that the listener will receive a list of records
  auto-startup: false # don't start the consumer automatically
  concurrency-level: 3 # means that 3 threads will be used to consume records
  session-timeout-ms: 10000 # means that the consumer will be considered dead after 10 seconds
  heartbeat-interval-ms: 3000 # means that the consumer will send a heartbeat every 3 seconds
  max-poll-interval-ms: 300000 # means that the consumer will be considered dead if it doesn't poll for 5 minutes
  max-poll-records: 500 # means that the consumer will poll for 500 records at a time
  max-partition-fetch-bytes-default: 1048576 # means that the consumer will fetch 1 MB of data from each partition
  max-partition-fetch-bytes-boost-factor: 1
  poll-timeout-ms: 300 # means that the consumer will wait for 300 milliseconds for records

elastic-config:
  index-name: news-index
  connection-url: http://localhost:9200
  connection-timeout-ms: 5000
  socket-timeout-ms: 30000
  is-repository: true
